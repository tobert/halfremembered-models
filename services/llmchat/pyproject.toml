[project]
name = "llmchat-service"
version = "2.0.0"
description = "OpenAI-compatible LLM service with tool calling support"
requires-python = ">=3.11,<3.14"
dependencies = [
    "torch",
    "torchaudio",
    "torchvision",
    "pytorch-triton-rocm",
    "transformers>=4.57.0",
    "accelerate",
    "qwen-vl-utils",
    "fastapi",
    "uvicorn[standard]",
    "httpx",
    "numpy",
    "pillow",
    "hrserve",
]

[project.optional-dependencies]
dev = ["pytest", "pytest-asyncio"]

[tool.uv]
package = false

[[tool.uv.index]]
name = "pytorch-rocm"
url = "https://download.pytorch.org/whl/nightly/rocm7.1"
explicit = true

[tool.uv.sources]
torch = { index = "pytorch-rocm" }
torchaudio = { index = "pytorch-rocm" }
torchvision = { index = "pytorch-rocm" }
pytorch-triton-rocm = { index = "pytorch-rocm" }
hrserve = { path = "../../hrserve", editable = true }

[tool.pytest.ini_options]
testpaths = ["tests"]
markers = [
    "slow: marks tests as slow (require model loading)",
]
